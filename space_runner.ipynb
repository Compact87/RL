{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T \n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "import socket\n",
    "import struct\n",
    "import time\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 8080    \n",
    "path='C:\\\\Users\\\\UrosV\\\\SpaceData\\\\tst.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "  return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "def receive(client):\n",
    "    buf = b''            # this caused an error, I added a b so it would be read as bytes and not string\n",
    "    while len(buf)<4:\n",
    "        buf += client.recv(4-len(buf))\n",
    "    size = struct.unpack('!i', buf)\n",
    "   # print(\"receiving %s bytes\" % size)\n",
    "\n",
    "    with open(path, 'wb') as img:\n",
    "        while True:\n",
    "            data = client.recv(1024)\n",
    "\n",
    "            if not data:\n",
    "                break\n",
    "            img.write(data)\n",
    "            \n",
    "            \n",
    "\n",
    "    #print('received image')\n",
    "#klasa neuralna mreza\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, img_height, img_width):\n",
    "        super().__init__()\n",
    "        \n",
    "       # self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "       # self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=1*img_height* img_width,out_features=24)\n",
    "        self.fc2=nn.Linear(in_features=24,out_features=32)\n",
    "    \n",
    "        self.out=nn.Linear(in_features=32, out_features=2)\n",
    "    def forward(self,t):\n",
    "        \n",
    "       # t=t\n",
    "         # (2) hidden conv layer\n",
    "       # t = self.conv1(t)\n",
    "       # print(t.shape)\n",
    "       # t = F.relu(t)\n",
    "        \n",
    "       # t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        #print(t.shape)\n",
    "        # (3) hidden conv layer\n",
    "       # t = self.conv2(t)\n",
    "       # t = F.relu(t)\n",
    "       # t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "       \n",
    "        \n",
    "        t=t.flatten(start_dim=1)# probati t=t.reshape(-1,img_height*img_width*3)\n",
    "        \n",
    "        t=F.relu(self.fc1(t))\n",
    "        t=F.relu(self.fc2(t))\n",
    "        #t=F.relu(self.fc3(t))\n",
    "        t=self.out(t)\n",
    "        return t\n",
    "\n",
    "#klasa experience     \n",
    "Experience=namedtuple(\n",
    "    'Experience',\n",
    "    ('state','action','next_state','reward')\n",
    ") \n",
    "\n",
    "#klasa ReplayMemory za storovanje experiensa\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity=capacity\n",
    "        self.memory=[]\n",
    "        self.push_count=0\n",
    "    def push(self, experience):\n",
    "        if len(self.memory)<self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count+=1    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        \n",
    "        return len(self.memory)>= batch_size\n",
    "    \n",
    "#klasa za izbor strategije explore/exploit    \n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start=start\n",
    "        self.end=end\n",
    "        self.decay=decay\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)\n",
    "\n",
    "#klasa za agenta\n",
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions,device):\n",
    "        self.current_step=0\n",
    "        self.strategy=strategy\n",
    "        self.num_actions=num_actions\n",
    "        self.device=device\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step+=1\n",
    "        \n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device) # exploit\n",
    "\n",
    "#klasa za upravljanje okruzenjem\n",
    "\n",
    "\n",
    "\n",
    "class CartPoleEnvManager():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "       # self.env=gym.make('CartPole-v0').unwrapped\n",
    "        self.reset()\n",
    "        self.imageReady=False\n",
    "        self.current_screen=None\n",
    "        self.done=False\n",
    "        self.actionNum=2\n",
    "    def reset(self):\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.connect((HOST, PORT))\n",
    "        sock.sendall(b\"x\\n\")\n",
    "        self.current_screen = None\n",
    "\n",
    "    def close(self):\n",
    "        #self.env.close()\n",
    "        self.socket.sendall(b\"c\\n\")\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode) \n",
    "    \n",
    "    def num_actions_available(self):\n",
    "        return self.actionNum\n",
    "    def take_action(self,action):\n",
    "        #_, reward, self.done, _=self.env.step(action.item())\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.connect((HOST, PORT))\n",
    "        \n",
    "        if action==0:\n",
    "            sock.sendall(b\"l\\n\")\n",
    "            receive(sock)\n",
    "            reward=self.get_reward()\n",
    "            done=self.get_done()\n",
    "        else:\n",
    "            sock.sendall(b\"r\\n\")\n",
    "            receive(sock)\n",
    "            reward=self.get_reward()\n",
    "            done=self.get_done()\n",
    "        \n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            print(\"starting\")\n",
    "            self.current_screen= self.get_processed_screen()            \n",
    "            black_screen=torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            #print(\"playing\")\n",
    "            s1=self.current_screen\n",
    "            s2=self.get_processed_screen()\n",
    "            self.current_screen=s2\n",
    "            \n",
    "            \n",
    "            return s2-s1\n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "\n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "    def get_processed_screen(self):\n",
    "        \n",
    "       # screen = self.render('rgb_array').transpose((2, 0, 1)) # PyTorch expects CHW\n",
    "        screen = np.array(Image.open(path)).transpose((2,0,1))\n",
    "        #print(screen)\n",
    "        #print(screen.shape)\n",
    "        #screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[1]\n",
    "\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.4)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        screen = screen[:, top:bottom, :]\n",
    "        return screen\n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Grayscale(num_output_channels=1),\n",
    "            T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)\n",
    "    def get_reward(self):\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.connect((HOST, PORT))\n",
    "        sock.sendall(b\"w\\n\")\n",
    "        reward = sock.recv(1024)\n",
    "        output=reward.decode(\"utf-8\")\n",
    "        return torch.tensor([int(output[2])], device=self.device)\n",
    "    def get_done(self):\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.connect((HOST, PORT))\n",
    "        sock.sendall(b\"d\\n\")\n",
    "        input = sock.recv(1024)\n",
    "        done=input.decode(\"utf-8\")\n",
    "        self.done=str2bool(done[2])\n",
    "        return self.done\n",
    "        \n",
    "        \n",
    "        \n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)\n",
    "\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                \n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        #print(final_state_locations)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        \n",
    "        batch_size = next_states.shape[0]\n",
    "       # print(non_final_state_locations)\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        #print(non_final_state_locations)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values\n",
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", \\\n",
    "        moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "starting\n",
      "episode done\n",
      "starting\n",
      "starting\n",
      "episode done\n",
      "starting\n",
      "starting\n",
      "episode done\n",
      "starting\n",
      "starting\n",
      "episode done\n",
      "starting\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a9aa3f6d994f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mExperience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-086ece8b6dce>\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m#_, reward, self.done, _=self.env.step(action.item())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0msock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAF_INET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHOST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPORT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "em = CartPoleEnvManager(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "episode_durations = []\n",
    "s=0\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    #plt.imshow(state)\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            #print('memory ready'+str(s))\n",
    "            s+=1\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "            #print(next_states.shape)\n",
    "           # print(rewards)\n",
    "           # print(actions)\n",
    "            \n",
    "            #print(states[0].shape)\n",
    "            plt.imshow(  states[1].permute(1, 2, 0)  )\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if em.done:\n",
    "            print('episode done')\n",
    "         #   episode_durations.append(timestep)\n",
    "          #  plot(episode_durations, 100)\n",
    "            break\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "em.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
