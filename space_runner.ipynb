{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T \n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "import socket\n",
    "import struct\n",
    "import time\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 8080    \n",
    "path='C:\\\\Users\\\\UrosV\\\\SpaceData\\\\tst.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def str2bool(v):\n",
    "  return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "# class for receiving image data\n",
    "def receive(client):\n",
    "    buf = b''            \n",
    "    while len(buf)<4:\n",
    "        buf += client.recv(4-len(buf))\n",
    "    size = struct.unpack('!i', buf)\n",
    "   # print(\"receiving %s bytes\" % size)\n",
    "\n",
    "    with open(path, 'wb') as img:\n",
    "        while True:\n",
    "            data = client.recv(1024)\n",
    "\n",
    "            if not data:\n",
    "                break\n",
    "            img.write(data)\n",
    "            \n",
    "            \n",
    "\n",
    "    #print('received image')\n",
    "#neural network class\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, img_height, img_width):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=6*7* 19,out_features=24)# use 1*img_height* img_width without conv layers\n",
    "        self.fc2=nn.Linear(in_features=24,out_features=32)\n",
    "    \n",
    "        self.out=nn.Linear(in_features=32, out_features=2)\n",
    "    def forward(self,t):\n",
    "        \n",
    "        t=t\n",
    "         # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)        \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        print(t.shape)\n",
    "        \n",
    "        t=t.flatten(start_dim=1)  # to try t=t.reshape(-1,img_height*img_width*3)\n",
    "        \n",
    "        t=F.relu(self.fc1(t))\n",
    "        t=F.relu(self.fc2(t))\n",
    "        #t=F.relu(self.fc3(t))\n",
    "        t=self.out(t)\n",
    "        return t\n",
    "\n",
    "# experience   \n",
    "Experience=namedtuple(\n",
    "    'Experience',\n",
    "    ('state','action','next_state','reward')\n",
    ") \n",
    "\n",
    "#class  ReplayMemory for experience storing\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity=capacity\n",
    "        self.memory=[]\n",
    "        self.push_count=0\n",
    "    def push(self, experience):\n",
    "        if len(self.memory)<self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count+=1    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        \n",
    "        return len(self.memory)>= batch_size\n",
    "    \n",
    "#class for choosing strategy \n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start=start\n",
    "        self.end=end\n",
    "        self.decay=decay\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)\n",
    "\n",
    "#agent class\n",
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions,device):\n",
    "        self.current_step=0\n",
    "        self.strategy=strategy\n",
    "        self.num_actions=num_actions\n",
    "        self.device=device\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step+=1\n",
    "        \n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device) # exploit\n",
    "\n",
    "#environment controll class, edited gym for use with java app\n",
    "\n",
    "\n",
    "\n",
    "class CartPoleEnvManager():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "       # self.env=gym.make('CartPole-v0').unwrapped\n",
    "        self.reset()\n",
    "        self.imageReady=False\n",
    "        self.current_screen=None\n",
    "        self.done=False\n",
    "        self.actionNum=2\n",
    "    def reset(self):\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.connect((HOST, PORT))\n",
    "        sock.sendall(b\"x\\n\")\n",
    "        self.current_screen = None\n",
    "\n",
    "    def close(self):\n",
    "        #self.env.close()\n",
    "        self.socket.sendall(b\"c\\n\")\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode) \n",
    "    \n",
    "    def num_actions_available(self):\n",
    "        return self.actionNum\n",
    "    def take_action(self,action):\n",
    "        #_, reward, self.done, _=self.env.step(action.item())\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.connect((HOST, PORT))\n",
    "        \n",
    "        if action==0:\n",
    "            sock.sendall(b\"l\\n\")\n",
    "            receive(sock)\n",
    "            reward=self.get_reward()\n",
    "            #done=self.get_done()\n",
    "        else:\n",
    "            sock.sendall(b\"r\\n\")\n",
    "            receive(sock)\n",
    "            reward=self.get_reward()\n",
    "            #done=self.get_done()\n",
    "        \n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            print(\"starting\")\n",
    "            self.current_screen= self.get_processed_screen()            \n",
    "            black_screen=torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1=self.current_screen\n",
    "            s2=self.get_processed_screen()\n",
    "            self.current_screen=s2\n",
    "            \n",
    "            \n",
    "            return s2-s1\n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "\n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "    def get_processed_screen(self):\n",
    "        \n",
    "       # screen = self.render('rgb_array').transpose((2, 0, 1)) # PyTorch expects CHW\n",
    "        screen = np.array(Image.open(path)).transpose((2,0,1))\n",
    "        #print(screen)\n",
    "        #print(screen.shape)\n",
    "        #screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[1]\n",
    "\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.4)\n",
    "        bottom = int(screen_height * 1)\n",
    "        screen = screen[:, top:bottom, :]\n",
    "        return screen\n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Grayscale(num_output_channels=1),\n",
    "            T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)\n",
    "    def get_reward(self):\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.connect((HOST, PORT))\n",
    "        sock.sendall(b\"w\\n\")\n",
    "        reward = sock.recv(1024)\n",
    "        output=reward.decode(\"utf-8\")\n",
    "        if int(output[2])==0:\n",
    "            em.done=True\n",
    "        return torch.tensor([int(output[2])], device=self.device)\n",
    "    def get_done(self):\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.connect((HOST, PORT))\n",
    "        sock.sendall(b\"d\\n\")\n",
    "        input = sock.recv(1024)\n",
    "        done=input.decode(\"utf-8\")\n",
    "        self.done=str2bool(done[2])\n",
    "        return self.done\n",
    "        \n",
    "        \n",
    "        \n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)\n",
    "\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                \n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        #print(final_state_locations)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        \n",
    "        batch_size = next_states.shape[0]      \n",
    "        values = torch.zeros(batch_size).to(QValues.device)       \n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values\n",
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", \\\n",
    "        moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.01\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "em = CartPoleEnvManager(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "episode_durations = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    s=0\n",
    "    print('reseting')\n",
    "    em.reset()\n",
    "    em.done=False\n",
    "    state = em.get_state()\n",
    "    #plt.imshow(state)\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)+s//10\n",
    "        print(reward)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        s+=1\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            #print('memory ready'+str(s))\n",
    "            \n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "            #print(next_states.shape)\n",
    "           # print(rewards)\n",
    "           # print(actions)\n",
    "            \n",
    "            #print(states[0].shape)\n",
    "            plt.imshow(  states[1].permute(1, 2, 0)  )\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if em.done:\n",
    "            print('episode done')\n",
    "            \n",
    "         #   episode_durations.append(timestep)\n",
    "          #  plot(episode_durations, 100)\n",
    "            break\n",
    "            \n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "em.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
